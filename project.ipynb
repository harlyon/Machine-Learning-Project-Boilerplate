# %% [markdown]
# # Machine Learning Project Boilerplate

# %% [markdown]
# ## 1. Importing the Libraries

# %%
# Basic Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning Libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Model Saving
import joblib

# GUI Libraries (Optional)
import tkinter as tk
from tkinter import ttk

# Set random seed for reproducibility
np.random.seed(42)

# %% [markdown]
# ## 2. Importing the Dataset

# %%
# Load your dataset
# df = pd.read_csv('your_dataset.csv')
# For demonstration, we'll create a dummy dataset
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)
df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(1, 11)])
df['target'] = y

# Display the first few rows
print("Dataset shape:", df.shape)
df.head()

# %% [markdown]
# ## 3. Taking Care of Missing Values

# %%
# Check for missing values
print("Missing values per column:")
print(df.isnull().sum())

# Handle missing values (if any)
if df.isnull().sum().any():
    # For numerical columns
    num_imputer = SimpleImputer(strategy='mean')
    num_cols = df.select_dtypes(include=['int64', 'float64']).columns
    df[num_cols] = num_imputer.fit_transform(df[num_cols])
    
    # For categorical columns (if any)
    cat_imputer = SimpleImputer(strategy='most_frequent')
    cat_cols = df.select_dtypes(include=['object']).columns
    if len(cat_cols) > 0:
        df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])

# Verify no missing values remain
print("\nMissing values after imputation:")
print(df.isnull().sum())

# %% [markdown]
# ## 4. Taking Care of Duplicate Values

# %%
# Check for duplicates
print("Number of duplicate rows:", df.duplicated().sum())

# Remove duplicates (if any)
if df.duplicated().sum() > 0:
    df = df.drop_duplicates()
    print("Duplicates removed. New shape:", df.shape)
else:
    print("No duplicates found.")

# %% [markdown]
# ## 5. Data Processing

# %%
# Exploratory Data Analysis
print("Basic Statistics:")
print(df.describe())

# Visualize distributions
plt.figure(figsize=(12, 6))
for i, col in enumerate(df.columns[:-1]):  # Exclude target
    plt.subplot(2, 5, i+1)
    sns.histplot(df[col], kde=True)
    plt.title(col)
plt.tight_layout()
plt.show()

# Correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0)
plt.title("Correlation Matrix")
plt.show()

# %% [markdown]
# ## 6. Encoding Categorical Data

# %%
# Check for categorical columns
cat_cols = df.select_dtypes(include=['object']).columns
if len(cat_cols) > 0:
    print("Categorical columns found:", list(cat_cols))
    
    # Label Encoding for binary categories
    le = LabelEncoder()
    for col in cat_cols:
        if df[col].nunique() == 2:
            df[col] = le.fit_transform(df[col])
            print(f"Label encoded column: {col}")
    
    # One-Hot Encoding for multi-categories
    remaining_cat_cols = df.select_dtypes(include=['object']).columns
    if len(remaining_cat_cols) > 0:
        df = pd.get_dummies(df, columns=remaining_cat_cols, drop_first=True)
        print(f"One-hot encoded columns: {list(remaining_cat_cols)}")
else:
    print("No categorical columns found.")

# Display processed dataframe
df.head()

# %% [markdown]
# ## 7. Feature Scaling

# %%
# Separate features and target
X = df.drop('target', axis=1)
y = df['target']

# Initialize scaler (StandardScaler or MinMaxScaler)
scaler = StandardScaler()
# scaler = MinMaxScaler()

# Scale features
X_scaled = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

# Display scaled features
print("Scaled Features:")
X_scaled.head()

# %% [markdown]
# ## 8. Splitting The Dataset Into The Training Set And Test Set

# %%
# Split the dataset (typically 80-20 or 70-30)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42)

print("Training set shape:", X_train.shape)
print("Test set shape:", X_test.shape)

# %% [markdown]
# ## 9. Logistic Regression

# %%
# Initialize and train the model
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)

# Predictions
y_pred_lr = lr.predict(X_test)

# Evaluation
print("Logistic Regression Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_lr))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_lr))

# %% [markdown]
# ## 10. SVC (Support Vector Classifier)

# %%
# Initialize and train the model
svc = SVC()
svc.fit(X_train, y_train)

# Predictions
y_pred_svc = svc.predict(X_test)

# Evaluation
print("SVC Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_svc))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_svc))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_svc))

# %% [markdown]
# ## 11. KNeighbors Classifier

# %%
# Initialize and train the model
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)

# Predictions
y_pred_knn = knn.predict(X_test)

# Evaluation
print("KNN Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_knn))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_knn))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_knn))

# %% [markdown]
# ## Non-Linear ML Algorithms

# %% [markdown]
# ### 12. Decision Tree Classifier

# %%
# Initialize and train the model
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

# Predictions
y_pred_dt = dt.predict(X_test)

# Evaluation
print("Decision Tree Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_dt))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_dt))

# Feature Importance
plt.figure(figsize=(10, 6))
feat_importances = pd.Series(dt.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.title("Feature Importance - Decision Tree")
plt.show()

# %% [markdown]
# ### 13. Random Forest Classifier

# %%
# Initialize and train the model
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

# Predictions
y_pred_rf = rf.predict(X_test)

# Evaluation
print("Random Forest Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_rf))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_rf))

# Feature Importance
plt.figure(figsize=(10, 6))
feat_importances = pd.Series(rf.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.title("Feature Importance - Random Forest")
plt.show()

# %% [markdown]
# ### 14. Gradient Boosting Classifier

# %%
# Initialize and train the model
gb = GradientBoostingClassifier(random_state=42)
gb.fit(X_train, y_train)

# Predictions
y_pred_gb = gb.predict(X_test)

# Evaluation
print("Gradient Boosting Performance:")
print("Accuracy:", accuracy_score(y_test, y_pred_gb))
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_gb))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_gb))

# Feature Importance
plt.figure(figsize=(10, 6))
feat_importances = pd.Series(gb.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.title("Feature Importance - Gradient Boosting")
plt.show()

# %% [markdown]
# ## 15. Prediction on New Data

# %%
# Example of making predictions on new data
# Create some new data (similar structure to training data)
new_data = pd.DataFrame(np.random.rand(5, 10), columns=X.columns)

# Preprocess the new data (same as training data)
new_data_scaled = scaler.transform(new_data)

# Make predictions using the best model (here using Random Forest as example)
new_predictions = rf.predict(new_data_scaled)
print("Predictions for new data:")
print(new_predictions)

# %% [markdown]
# ## 16. Save Model Using Joblib

# %%
# Save the best performing model and scaler
joblib.dump(rf, 'random_forest_model.joblib')  # Example using Random Forest
joblib.dump(scaler, 'scaler.joblib')

print("Model and scaler saved successfully.")

# To load later:
# loaded_model = joblib.load('random_forest_model.joblib')
# loaded_scaler = joblib.load('scaler.joblib')

# %% [markdown]
# ## 17. Creating GUI (Optional)

# %%
# Simple GUI for model prediction (uncomment to use)
"""
def predict_gui():
    # Load the saved model and scaler
    model = joblib.load('random_forest_model.joblib')
    scaler = joblib.load('scaler.joblib')
    
    # Get values from GUI
    input_values = []
    for entry in entries:
        input_values.append(float(entry.get()))
    
    # Convert to DataFrame and scale
    input_df = pd.DataFrame([input_values], columns=X.columns)
    input_scaled = scaler.transform(input_df)
    
    # Make prediction
    prediction = model.predict(input_scaled)
    result_label.config(text=f"Prediction: {'Class 1' if prediction[0] == 1 else 'Class 0'}")

# Create main window
root = tk.Tk()
root.title("ML Model Predictor")

# Create input fields
entries = []
for i, col in enumerate(X.columns):
    ttk.Label(root, text=col).grid(row=i, column=0, padx=5, pady=5)
    entry = ttk.Entry(root)
    entry.grid(row=i, column=1, padx=5, pady=5)
    entries.append(entry)

# Prediction button
predict_btn = ttk.Button(root, text="Predict", command=predict_gui)
predict_btn.grid(row=len(X.columns), column=0, columnspan=2, pady=10)

# Result label
result_label = ttk.Label(root, text="Prediction: ")
result_label.grid(row=len(X.columns)+1, column=0, columnspan=2)

root.mainloop()
"""
